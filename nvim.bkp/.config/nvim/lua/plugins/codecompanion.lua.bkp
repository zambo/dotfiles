return {
  {
    "olimorris/codecompanion.nvim",
    dependencies = {
      "nvim-lua/plenary.nvim",
      "nvim-treesitter/nvim-treesitter",
      "ravitemer/codecompanion-history.nvim",
    },
    opts = {
      -- NOTE: The log_level is in `opts.opts`
      opts = {
        log_level = "DEBUG", -- or "TRACE"
      },
      adapters = {
        http = {
          fuel_ix = function()
            local openai = require("codecompanion.adapters.http.openai")
            return {
              -- something
              name = "fuel_ix",
              formatted_name = "Fuel iX",
              roles = {
                llm = "assistant",
                user = "user",
              },
              opts = {
                stream = true,
                vision = false,
              },
              features = {
                text = true,
                tokens = true,
              },
              url = "https://api.fuelix.ai/v1/chat/completions",
              env = {
                api_key = "cmd:op read 'op://Private/Fuel iX - Default API Key/API Key' --no-newline",
              },
              headers = {
                Authorization = "Bearer ${api_key}",
                ["Content-Type"] = "application/json",
              },
              handlers = {
                setup = function(self)
                  if self.opts and self.opts.stream then
                    self.parameters.stream = true
                  end
                  return true
                end,

                --- Use the OpenAI adapter for the bulk of the work
                tokens = function(self, data)
                  return openai.handlers.tokens(self, data)
                end,
                form_parameters = function(self, params, messages)
                  return openai.handlers.form_parameters(self, params, messages)
                end,
                form_messages = function(self, messages)
                  return openai.handlers.form_messages(self, messages)
                end,
                chat_output = function(self, data)
                  return openai.handlers.chat_output(self, data)
                end,
                inline_output = function(self, data, context)
                  return openai.handlers.inline_output(self, data, context)
                end,
                on_exit = function(self, data)
                  return openai.handlers.on_exit(self, data)
                end,
              },
              schema = {
                ---@type CodeCompanion.Schema
                model = {
                  order = 1,
                  mapping = "parameters",
                  type = "enum",
                  desc = "ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.",
                  default = "claude-haiku-4-5",
                  choices = {
                    "claude-haiku-4-5",
                    "claude-sonnet-4",
                  },
                },
              },
            }
          end,
        },
      },
      extensions = {
        history = {
          enabled = true,
          opts = {
            keymap = "<Leader>ah",
            save_chat_keymap = "<Leader>as",
            auto_save = true,
            expiration_days = 0,
            picker = "snacks",
            auto_generate_title = true,
            continue_last_chat = false,
            delete_on_clearing_chat = false,
            dir_to_save = vim.fn.stdpath("data") .. "/codecompanion-history",
            enable_logging = false,
          },
        },
      },
    },
  },
}
